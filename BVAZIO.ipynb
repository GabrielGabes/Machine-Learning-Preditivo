{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699d88a7",
   "metadata": {},
   "source": [
    "# Funções, bibliotecas e Dataframe ficticios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b81531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import matplotlib.font_manager as fm #para alterar as fontes dos graficos\n",
    "import seaborn as sns\n",
    "\n",
    "import pyperclip\n",
    "\n",
    "# Definindo o estilo de fonte e tema\n",
    "fonte = fm.FontProperties(family=\"Calibri\", style=\"italic\")\n",
    "\n",
    "sns.set_style('whitegrid') #cor da grade(fundo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3960e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação e Tratamento de dados\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import NaN\n",
    "\n",
    "#ignorando Warning inuteis\n",
    "import warnings \n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4724bd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/GabrielGabes/functions_gsa/main/fake_database.py\n",
      "https://raw.githubusercontent.com/GabrielGabes/functions_gsa/main/funcoes_estatisticas.py\n",
      "https://raw.githubusercontent.com/GabrielGabes/functions_gsa/main/ML_supervised_learning.py\n",
      "TUDO OK\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# CARREGANDO BASE\n",
    "arquivo = 'fake_database'\n",
    "url = \"https://raw.githubusercontent.com/GabrielGabes/functions_gsa/main/\" + arquivo + \".py\"\n",
    "print(url)\n",
    "#response = requests.get(url); code = response.text; exec(code)\n",
    "#df = fake_database2(2250); df_bck = df.copy(); display(df.head())\n",
    "\n",
    "arquivo = 'funcoes_estatisticas'\n",
    "url = \"https://raw.githubusercontent.com/GabrielGabes/functions_gsa/main/\" + arquivo + \".py\"\n",
    "print(url)\n",
    "response = requests.get(url); code = response.text; exec(code)\n",
    "\n",
    "# Função para avaliação de modelos exibindo metricas de avaliação\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, RocCurveDisplay\n",
    "arquivo = 'ML_supervised_learning'\n",
    "url = \"https://raw.githubusercontent.com/GabrielGabes/functions_gsa/main/\" + arquivo + \".py\"\n",
    "print(url); response = requests.get(url); code = response.text; exec(code)\n",
    "\n",
    "print('TUDO OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "443fdfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contribuicao_proba_tabela(df, coluna_analisada, model, sep=10):\n",
    "    # Criar DataFrame para análise com a variável de interesse variando em um intervalo\n",
    "    minimo, maximo = df[coluna_analisada].min(), df[coluna_analisada].max()\n",
    "    df_analise_feature = pd.DataFrame()\n",
    "\n",
    "    # Preencher as demais colunas com o valor médio\n",
    "    for coluna in df.columns:\n",
    "        if coluna != coluna_analisada:\n",
    "            df_analise_feature[coluna] = df[coluna].mean()\n",
    "        else:\n",
    "            df_analise_feature[coluna] = np.linspace(minimo, maximo, sep)\n",
    "\n",
    "    # Fazer previsão de probabilidade e adicionar ao DataFrame de análise\n",
    "    df_analise_feature['predict_proba'] = model.predict_proba(df_analise_feature)[:, 1]\n",
    "\n",
    "    # Retornar apenas a coluna analisada e a probabilidade prevista\n",
    "    return df_analise_feature[[coluna_analisada, 'predict_proba']]\n",
    "\n",
    "#x_data = pd.DataFrame(x_teste, columns=df.columns[:-1])\n",
    "#resultado = contribuicao_proba_tabela(x_data, 'feature_4', modelo_m1, sep=10)\n",
    "#resultado\n",
    "\n",
    "def contribuicao_proba_grafico(resultado, coluna):\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    norm = plt.Normalize(resultado['predict_proba'].min(), resultado['predict_proba'].max())\n",
    "    colors = cmap(norm(resultado['predict_proba']))\n",
    "\n",
    "    ax = sns.lineplot(x=resultado[coluna].round(1), y=resultado['predict_proba'])\n",
    "\n",
    "#plt.figure(figsize=(4,3))\n",
    "#contribuicao_proba_grafico(resultado, 'feature_4')\n",
    "#plt.show()\n",
    "\n",
    "def contribuicao_proba(x_data, modelo, sep=10):\n",
    "    for i, coluna_analisada in enumerate(x_data.columns, 1):\n",
    "        plt.subplot(10, 3, i)\n",
    "        resultado = contribuicao_proba_tabela(x_data, coluna_analisada, modelo, sep=sep)\n",
    "        contribuicao_proba_grafico(resultado, coluna_analisada)\n",
    "\n",
    "\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "def contribuicao_proba2(x_data, modelo):\n",
    "    # Definir o tamanho da figura geral\n",
    "    fig, axes = plt.subplots(len(x_data.columns)//3+1, 3, figsize=(10, 10))\n",
    "    axes = axes.ravel()  # Transformar a matriz de eixos em uma lista para fácil iteração\n",
    "\n",
    "    # Iterar pelas colunas e gerar gráficos de dependência parcial\n",
    "    for i, column in enumerate(x_data.columns):\n",
    "        PartialDependenceDisplay.from_estimator(modelo, x_data, [column], ax=axes[i])\n",
    "\n",
    "    # Ajustar o layout para evitar sobreposição\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824529d",
   "metadata": {},
   "source": [
    "# Criando dataframe sintetico desbalanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87342e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Gerar dados sintéticos desbalanceados\n",
    "x, y = make_classification(n_samples=7000, \n",
    "                           n_features=10, # qtd total de features\n",
    "                           n_informative=6, # features realmente uteis para o modelo\n",
    "                           n_redundant=3, # features que diz praticamente oque uma outra já fim\n",
    "                           n_classes=2, \n",
    "                           n_clusters_per_class=3,  # Subgrupos\n",
    "                           weights=[0.93, 0.07], \n",
    "                           class_sep=0.2, # separação entre as classes\n",
    "                           random_state=3141592)\n",
    "\n",
    "# Criar um DataFrame\n",
    "df = pd.DataFrame(x, columns=[f'feature_{i}' for i in range(x.shape[1])])\n",
    "df['target'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f2a05",
   "metadata": {},
   "source": [
    "# Tratando o DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2c6d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5250, 10) (1750, 10)\n",
      "(5250,) (1750,)\n",
      "   count  proportion\n",
      "0   4865   92.666667\n",
      "1    385    7.333333\n",
      "   count  proportion\n",
      "0   1622   92.685714\n",
      "1    128    7.314286\n"
     ]
    }
   ],
   "source": [
    "grandom_state = 3141592\n",
    "\n",
    "# Variavel Dependente\n",
    "var_dep = 'target'\n",
    "y = df[var_dep]\n",
    "x = df.drop(var_dep, axis=1)\n",
    "\n",
    "############################################################################################\n",
    "# DUMMYRIZAÇÃO\n",
    "colunas_categoricas = []\n",
    "colunas_binarias = []\n",
    "colunas_mais3_categorias = []\n",
    "colunas_numericas = []\n",
    "\n",
    "for coluna in x.columns:\n",
    "    if df[coluna].dtype == 'O':\n",
    "        colunas_categoricas.append(coluna)\n",
    "\n",
    "        categorias = x[coluna].unique()\n",
    "        if len(categorias) == 2:\n",
    "            print('2 niveis:', coluna, '=>', categorias)\n",
    "            colunas_binarias.append(coluna)\n",
    "        else:\n",
    "            print('3 niveis:', coluna, '=>', categorias)\n",
    "            colunas_mais3_categorias.append(coluna)\n",
    "    else:\n",
    "        colunas_numericas.append(coluna)\n",
    "\n",
    "############################################################################################\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder #transformando colunas com 2 categorias em 0 e 1\n",
    "\n",
    "coluna = x.columns\n",
    "one_hot = make_column_transformer((\n",
    "    OneHotEncoder(drop='if_binary'), #caso a coluna tenha apenas 2 categorias \n",
    "    colunas_categoricas), #passando quais são essas colunas\n",
    "    remainder = 'passthrough', sparse_threshold=0) #oque deve ser feito com as outras\n",
    "\n",
    "#Aplicando transformação\n",
    "x = one_hot.fit_transform(x)\n",
    "\n",
    "#Os novos nomes das colunas #'onehotencoder=transformadas; 'remainder'=não transformadas\n",
    "novos_nomes_colunas = one_hot.get_feature_names_out(coluna)\n",
    "\n",
    "# Remover prefixo 'remainder__' das colunas que não foram transformadas\n",
    "#novos_nomes_colunas = [nome.replace('remainder__', '') for nome in novos_nomes_colunas]\n",
    "\n",
    "x = pd.DataFrame(x, columns = novos_nomes_colunas) #alterando de volta\n",
    "x_columns = x.columns.tolist() \n",
    "\n",
    "############################################################################################\n",
    "# Normalização (scaling entre 0 e 1) com MinMaxScaler ******************************\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalizacao = MinMaxScaler()\n",
    "#x = normalizacao.fit_transform(x)\n",
    "# df['Close_normalizada'] = (df[coluna] - df[coluna].min()) / (df[coluna].max() - df[coluna].min())\n",
    "\n",
    "# Padronização (média 0 e desvio padrão 1) com StandardScaler **********************\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "padronizacao = StandardScaler()\n",
    "#x = padronizacao.fit_transform(x)\n",
    "# df['Close_padronizada'] = (df[coluna] - df[coluna].mean()) / df[coluna].std()\n",
    "\n",
    "############################################################################################\n",
    "# DEFININDO A VARIAVEL DEPENDENTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "############################################################################################\n",
    "#backups\n",
    "x_inteiro = x\n",
    "y_inteiro = y\n",
    "\n",
    "# DIVIDINDO BASE EM TREINO E TESTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, \n",
    "                                                    stratify = y, #para manter a proporção da Var Dep nos splits\n",
    "                                                    random_state = grandom_state) #raiz da aleatoridade\n",
    "# test_size = 0.25 #porcentagem que ira ser separado para testes\n",
    "\n",
    "print(x_treino.shape, x_teste.shape)\n",
    "\n",
    "print(y_treino.shape, y_teste.shape)\n",
    "print(pd.concat([pd.Series(y_treino).value_counts(), pd.Series(y_treino).value_counts(normalize=True)*100], axis=1))\n",
    "print(pd.concat([pd.Series(y_teste).value_counts(), pd.Series(y_teste).value_counts(normalize=True)*100], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b70dfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e4c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef70d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d15d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a6c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03afb329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1dba9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d8bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a96a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205b4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d49d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
